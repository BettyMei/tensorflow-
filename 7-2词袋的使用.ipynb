{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFyCAYAAACds1IeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf9a8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_file_name = os.path.join('temp','temp_spam_data.csv')\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name,'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    \n",
    "    with open(save_file_name,'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "target = [1 if x=='spam' else 0 for x in target]\n",
    "\n",
    "texts = [x.lower() for x in texts] #转换为小写\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts] #移除标点符号\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts] #移除数字\n",
    "texts = [' '.join(x.split()) for x in texts] #移除空白\n",
    "#计算最长句子大小\n",
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths,bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "sentence_size = 25\n",
    "min_word_freq = 3    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow自带分词器VocabularyProcessor()\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size,min_frequency=min_word_freq)\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)\n",
    "#分割数据集为训练集和测试集\n",
    "train_indices = np.random.choice(len(texts),round(len(texts)*0.8),replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts)))-set(train_indices)))\n",
    "texts_train = [x for ix,x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix,x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix,x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix,x in enumerate(target) if ix in test_indices]\n",
    "#声明词嵌入矩阵\n",
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
    "\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "x_data = tf.placeholder(shape=[sentence_size],dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1,1],dtype=tf.float32)\n",
    "\n",
    "x_embed = tf.nn.embedding_lookup(identity_mat,x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed,0)\n",
    "\n",
    "x_col_sums_2D = tf.expand_dims(x_col_sums,0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D,A),b)\n",
    "\n",
    "#声明训练模型的损失函数、预测函数和优化器\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output,labels=y_target))\n",
    "prediction = tf.sigmoid(model_output)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Training Observation #10: Loss = 9.545351\n",
      "Training Observation #20: Loss = 5.88455\n",
      "Training Observation #30: Loss = 3.9939697\n",
      "Training Observation #40: Loss = 3.5798464\n",
      "Training Observation #50: Loss = 0.00023631282\n",
      "Training Observation #60: Loss = 2.7195547\n",
      "Training Observation #70: Loss = 0.121294245\n",
      "Training Observation #80: Loss = 0.62758905\n",
      "Training Observation #90: Loss = 0.21000326\n",
      "Training Observation #100: Loss = 2.6374543\n",
      "Training Observation #110: Loss = 0.0031235858\n",
      "Training Observation #120: Loss = 0.004683068\n",
      "Training Observation #130: Loss = 4.9761118e-05\n",
      "Training Observation #140: Loss = 0.9864392\n",
      "Training Observation #150: Loss = 0.3307791\n",
      "Training Observation #160: Loss = 0.00049460505\n",
      "Training Observation #170: Loss = 1.040765e-05\n",
      "Training Observation #180: Loss = 0.22561485\n",
      "Training Observation #190: Loss = 0.8007037\n",
      "Training Observation #200: Loss = 6.1761985\n",
      "Training Observation #210: Loss = 8.344412e-05\n",
      "Training Observation #220: Loss = 0.4240606\n",
      "Training Observation #230: Loss = 0.0033533722\n",
      "Training Observation #240: Loss = 5.894732\n",
      "Training Observation #250: Loss = 0.0051858635\n",
      "Training Observation #260: Loss = 0.024302427\n",
      "Training Observation #270: Loss = 0.000574856\n",
      "Training Observation #280: Loss = 0.0007504987\n",
      "Training Observation #290: Loss = 5.258963\n",
      "Training Observation #300: Loss = 11.566265\n",
      "Training Observation #310: Loss = 2.7761204\n",
      "Training Observation #320: Loss = 0.000696046\n",
      "Training Observation #330: Loss = 0.014675493\n",
      "Training Observation #340: Loss = 1.6737099\n",
      "Training Observation #350: Loss = 0.00021095677\n",
      "Training Observation #360: Loss = 0.0025095812\n",
      "Training Observation #370: Loss = 3.0107854e-05\n",
      "Training Observation #380: Loss = 0.00016691654\n",
      "Training Observation #390: Loss = 0.00010235925\n",
      "Training Observation #400: Loss = 0.00013460583\n",
      "Training Observation #410: Loss = 0.18815334\n",
      "Training Observation #420: Loss = 0.00016885866\n",
      "Training Observation #430: Loss = 0.054892223\n",
      "Training Observation #440: Loss = 6.0826555e-05\n",
      "Training Observation #450: Loss = 0.0027251104\n",
      "Training Observation #460: Loss = 0.0014884879\n",
      "Training Observation #470: Loss = 2.6978718e-05\n",
      "Training Observation #480: Loss = 0.0011675076\n",
      "Training Observation #490: Loss = 0.065458015\n",
      "Training Observation #500: Loss = 0.0007989821\n",
      "Training Observation #510: Loss = 0.0016218473\n",
      "Training Observation #520: Loss = 6.553181e-05\n",
      "Training Observation #530: Loss = 0.0065441094\n",
      "Training Observation #540: Loss = 0.05796349\n",
      "Training Observation #550: Loss = 0.53703076\n",
      "Training Observation #560: Loss = 14.495711\n",
      "Training Observation #570: Loss = 0.00079887547\n",
      "Training Observation #580: Loss = 0.0104167145\n",
      "Training Observation #590: Loss = 0.006624986\n",
      "Training Observation #600: Loss = 0.0008756328\n",
      "Training Observation #610: Loss = 5.8162093\n",
      "Training Observation #620: Loss = 9.089299e-05\n",
      "Training Observation #630: Loss = 0.000557491\n",
      "Training Observation #640: Loss = 1.4355551\n",
      "Training Observation #650: Loss = 10.444061\n",
      "Training Observation #660: Loss = 4.1784034\n",
      "Training Observation #670: Loss = 0.36319315\n",
      "Training Observation #680: Loss = 3.5375423\n",
      "Training Observation #690: Loss = 0.00012843311\n",
      "Training Observation #700: Loss = 0.19934008\n",
      "Training Observation #710: Loss = 0.00048001562\n",
      "Training Observation #720: Loss = 4.760091e-06\n",
      "Training Observation #730: Loss = 0.00015894468\n",
      "Training Observation #740: Loss = 0.49061927\n",
      "Training Observation #750: Loss = 0.00054256315\n",
      "Training Observation #760: Loss = 0.00029074558\n",
      "Training Observation #770: Loss = 0.018646536\n",
      "Training Observation #780: Loss = 7.979738\n",
      "Training Observation #790: Loss = 4.5282545\n",
      "Training Observation #800: Loss = 6.455438e-06\n",
      "Training Observation #810: Loss = 0.13297343\n",
      "Training Observation #820: Loss = 0.00020927996\n",
      "Training Observation #830: Loss = 0.00037299472\n",
      "Training Observation #840: Loss = 0.0023989573\n",
      "Training Observation #850: Loss = 0.008771457\n",
      "Training Observation #860: Loss = 7.9999566\n",
      "Training Observation #870: Loss = 6.698636\n",
      "Training Observation #880: Loss = 2.144023\n",
      "Training Observation #890: Loss = 2.2734842\n",
      "Training Observation #900: Loss = 1.8933053e-05\n",
      "Training Observation #910: Loss = 0.45987368\n",
      "Training Observation #920: Loss = 0.030188251\n",
      "Training Observation #930: Loss = 0.00017790984\n",
      "Training Observation #940: Loss = 0.00013370543\n",
      "Training Observation #950: Loss = 5.5069213\n",
      "Training Observation #960: Loss = 1.0794225\n",
      "Training Observation #970: Loss = 0.17669727\n",
      "Training Observation #980: Loss = 3.6130342\n",
      "Training Observation #990: Loss = 0.0033003276\n",
      "Training Observation #1000: Loss = 0.00016455397\n",
      "Training Observation #1010: Loss = 0.015782526\n",
      "Training Observation #1020: Loss = 3.504057e-05\n",
      "Training Observation #1030: Loss = 0.0007491109\n",
      "Training Observation #1040: Loss = 0.0023748449\n",
      "Training Observation #1050: Loss = 0.11549144\n",
      "Training Observation #1060: Loss = 3.0974429\n",
      "Training Observation #1070: Loss = 5.9385514\n",
      "Training Observation #1080: Loss = 0.032139234\n",
      "Training Observation #1090: Loss = 5.9064078e-06\n",
      "Training Observation #1100: Loss = 0.05694262\n",
      "Training Observation #1110: Loss = 2.723386e-07\n",
      "Training Observation #1120: Loss = 0.047069468\n",
      "Training Observation #1130: Loss = 1.0851161\n",
      "Training Observation #1140: Loss = 0.19709714\n",
      "Training Observation #1150: Loss = 1.4526825e-05\n",
      "Training Observation #1160: Loss = 0.0010096786\n",
      "Training Observation #1170: Loss = 3.1406205\n",
      "Training Observation #1180: Loss = 0.00051383773\n",
      "Training Observation #1190: Loss = 6.260414e-05\n",
      "Training Observation #1200: Loss = 5.8944774e-05\n",
      "Training Observation #1210: Loss = 0.0009919074\n",
      "Training Observation #1220: Loss = 0.025731266\n",
      "Training Observation #1230: Loss = 3.9412835e-06\n",
      "Training Observation #1240: Loss = 0.00018395472\n",
      "Training Observation #1250: Loss = 0.0003636366\n",
      "Training Observation #1260: Loss = 3.695682e-05\n",
      "Training Observation #1270: Loss = 9.709654e-07\n",
      "Training Observation #1280: Loss = 9.079673e-07\n",
      "Training Observation #1290: Loss = 7.22275e-07\n",
      "Training Observation #1300: Loss = 0.0016856601\n",
      "Training Observation #1310: Loss = 1.8400146e-06\n",
      "Training Observation #1320: Loss = 0.057045937\n",
      "Training Observation #1330: Loss = 0.00034254306\n",
      "Training Observation #1340: Loss = 1.2302468\n",
      "Training Observation #1350: Loss = 3.9515842e-05\n",
      "Training Observation #1360: Loss = 0.024410946\n",
      "Training Observation #1370: Loss = 0.00012523784\n",
      "Training Observation #1380: Loss = 0.015795823\n",
      "Training Observation #1390: Loss = 5.034628e-05\n",
      "Training Observation #1400: Loss = 0.0034465173\n",
      "Training Observation #1410: Loss = 0.42404807\n",
      "Training Observation #1420: Loss = 0.0060881767\n",
      "Training Observation #1430: Loss = 0.0011018391\n",
      "Training Observation #1440: Loss = 2.7752485\n",
      "Training Observation #1450: Loss = 11.288032\n",
      "Training Observation #1460: Loss = 12.971655\n",
      "Training Observation #1470: Loss = 7.142613e-05\n",
      "Training Observation #1480: Loss = 2.8630776\n",
      "Training Observation #1490: Loss = 2.1482944\n",
      "Training Observation #1500: Loss = 0.0008491579\n",
      "Training Observation #1510: Loss = 5.3810387\n",
      "Training Observation #1520: Loss = 0.22650047\n",
      "Training Observation #1530: Loss = 0.36450168\n",
      "Training Observation #1540: Loss = 6.840368\n",
      "Training Observation #1550: Loss = 3.587121e-05\n",
      "Training Observation #1560: Loss = 0.06170651\n",
      "Training Observation #1570: Loss = 3.8977733\n",
      "Training Observation #1580: Loss = 0.00037905137\n",
      "Training Observation #1590: Loss = 7.8669386\n",
      "Training Observation #1600: Loss = 0.00055337447\n",
      "Training Observation #1610: Loss = 1.3113644\n",
      "Training Observation #1620: Loss = 1.1941903e-05\n",
      "Training Observation #1630: Loss = 0.0014311352\n",
      "Training Observation #1640: Loss = 0.0007967835\n",
      "Training Observation #1650: Loss = 6.7530665\n",
      "Training Observation #1660: Loss = 0.00038811532\n",
      "Training Observation #1670: Loss = 0.0006206092\n",
      "Training Observation #1680: Loss = 0.00066421804\n",
      "Training Observation #1690: Loss = 6.5237323e-06\n",
      "Training Observation #1700: Loss = 0.000118587894\n",
      "Training Observation #1710: Loss = 9.710801e-06\n",
      "Training Observation #1720: Loss = 6.9566435e-05\n",
      "Training Observation #1730: Loss = 1.2111545e-05\n",
      "Training Observation #1740: Loss = 0.011137677\n",
      "Training Observation #1750: Loss = 3.920539\n",
      "Training Observation #1760: Loss = 2.2524769e-06\n",
      "Training Observation #1770: Loss = 2.0686784\n",
      "Training Observation #1780: Loss = 5.8499527\n",
      "Training Observation #1790: Loss = 0.0058479477\n",
      "Training Observation #1800: Loss = 6.453665e-06\n",
      "Training Observation #1810: Loss = 3.8587104e-06\n",
      "Training Observation #1820: Loss = 2.3204291e-05\n",
      "Training Observation #1830: Loss = 0.0006857003\n",
      "Training Observation #1840: Loss = 0.0013517059\n",
      "Training Observation #1850: Loss = 0.08762842\n",
      "Training Observation #1860: Loss = 3.348159\n",
      "Training Observation #1870: Loss = 3.270009e-05\n",
      "Training Observation #1880: Loss = 0.00011228547\n",
      "Training Observation #1890: Loss = 7.103596\n",
      "Training Observation #1900: Loss = 0.00057053665\n",
      "Training Observation #1910: Loss = 2.0730333\n",
      "Training Observation #1920: Loss = 1.1459627e-05\n",
      "Training Observation #1930: Loss = 5.81512\n",
      "Training Observation #1940: Loss = 0.00017735745\n",
      "Training Observation #1950: Loss = 6.03652\n",
      "Training Observation #1960: Loss = 2.1718413e-06\n",
      "Training Observation #1970: Loss = 0.0023204125\n",
      "Training Observation #1980: Loss = 0.007795593\n",
      "Training Observation #1990: Loss = 0.28110626\n",
      "Training Observation #2000: Loss = 9.181609e-05\n",
      "Training Observation #2010: Loss = 0.015398184\n",
      "Training Observation #2020: Loss = 0.025332518\n",
      "Training Observation #2030: Loss = 2.9742315\n",
      "Training Observation #2040: Loss = 6.0261995e-05\n",
      "Training Observation #2050: Loss = 1.7520409e-06\n",
      "Training Observation #2060: Loss = 0.69832677\n",
      "Training Observation #2070: Loss = 0.0006167529\n",
      "Training Observation #2080: Loss = 0.00016839585\n",
      "Training Observation #2090: Loss = 3.9192494e-07\n",
      "Training Observation #2100: Loss = 2.5495683e-06\n",
      "Training Observation #2110: Loss = 2.241813e-06\n",
      "Training Observation #2120: Loss = 3.1089919\n",
      "Training Observation #2130: Loss = 0.0015714677\n",
      "Training Observation #2140: Loss = 0.4565484\n",
      "Training Observation #2150: Loss = 4.2785254e-05\n",
      "Training Observation #2160: Loss = 0.0012203291\n",
      "Training Observation #2170: Loss = 5.9142636e-05\n",
      "Training Observation #2180: Loss = 5.5279146e-05\n",
      "Training Observation #2190: Loss = 1.4922886\n",
      "Training Observation #2200: Loss = 3.8350343e-05\n",
      "Training Observation #2210: Loss = 0.59223163\n",
      "Training Observation #2220: Loss = 0.01515128\n",
      "Training Observation #2230: Loss = 8.6661006e-05\n",
      "Training Observation #2240: Loss = 3.4187794\n",
      "Training Observation #2250: Loss = 0.000104137886\n",
      "Training Observation #2260: Loss = 0.0069910483\n",
      "Training Observation #2270: Loss = 0.67515236\n",
      "Training Observation #2280: Loss = 3.302567\n",
      "Training Observation #2290: Loss = 0.002212704\n",
      "Training Observation #2300: Loss = 0.25198582\n",
      "Training Observation #2310: Loss = 1.6188545e-05\n",
      "Training Observation #2320: Loss = 1.6568213e-06\n",
      "Training Observation #2330: Loss = 4.940494\n",
      "Training Observation #2340: Loss = 0.12406446\n",
      "Training Observation #2350: Loss = 0.00012749045\n",
      "Training Observation #2360: Loss = 1.3391109e-05\n",
      "Training Observation #2370: Loss = 2.7110944\n",
      "Training Observation #2380: Loss = 4.2480166e-05\n",
      "Training Observation #2390: Loss = 7.8334034e-07\n",
      "Training Observation #2400: Loss = 9.274007e-05\n",
      "Training Observation #2410: Loss = 0.00042155726\n",
      "Training Observation #2420: Loss = 0.0008392562\n",
      "Training Observation #2430: Loss = 1.7766569\n",
      "Training Observation #2440: Loss = 0.0009997952\n",
      "Training Observation #2450: Loss = 0.0041387887\n",
      "Training Observation #2460: Loss = 0.013919825\n",
      "Training Observation #2470: Loss = 2.244288e-05\n",
      "Training Observation #2480: Loss = 6.760414\n",
      "Training Observation #2490: Loss = 7.441049\n",
      "Training Observation #2500: Loss = 0.0019093758\n",
      "Training Observation #2510: Loss = 0.16355744\n",
      "Training Observation #2520: Loss = 1.9130797e-05\n",
      "Training Observation #2530: Loss = 0.86743385\n",
      "Training Observation #2540: Loss = 0.00023739423\n",
      "Training Observation #2550: Loss = 0.0022643483\n",
      "Training Observation #2560: Loss = 0.0023337326\n",
      "Training Observation #2570: Loss = 8.9401234e-05\n",
      "Training Observation #2580: Loss = 0.0046955366\n",
      "Training Observation #2590: Loss = 0.0023300122\n",
      "Training Observation #2600: Loss = 0.5363135\n",
      "Training Observation #2610: Loss = 4.49203\n",
      "Training Observation #2620: Loss = 0.20577155\n",
      "Training Observation #2630: Loss = 0.0072818794\n",
      "Training Observation #2640: Loss = 3.7162807\n",
      "Training Observation #2650: Loss = 0.051708344\n",
      "Training Observation #2660: Loss = 1.1893736e-06\n",
      "Training Observation #2670: Loss = 0.0010890913\n",
      "Training Observation #2680: Loss = 1.8260978e-06\n",
      "Training Observation #2690: Loss = 3.0343932e-05\n",
      "Training Observation #2700: Loss = 0.0383587\n",
      "Training Observation #2710: Loss = 0.0026097044\n",
      "Training Observation #2720: Loss = 6.495235\n",
      "Training Observation #2730: Loss = 1.6374734e-06\n",
      "Training Observation #2740: Loss = 2.3722141e-05\n",
      "Training Observation #2750: Loss = 6.7631045e-06\n",
      "Training Observation #2760: Loss = 0.000119354845\n",
      "Training Observation #2770: Loss = 0.00016065335\n",
      "Training Observation #2780: Loss = 0.11623723\n",
      "Training Observation #2790: Loss = 0.00013835636\n",
      "Training Observation #2800: Loss = 3.7617571\n",
      "Training Observation #2810: Loss = 7.795627e-07\n",
      "Training Observation #2820: Loss = 0.00010826933\n",
      "Training Observation #2830: Loss = 0.0007677142\n",
      "Training Observation #2840: Loss = 0.0002237426\n",
      "Training Observation #2850: Loss = 0.00028174373\n",
      "Training Observation #2860: Loss = 3.8149392e-05\n",
      "Training Observation #2870: Loss = 1.6205398e-05\n",
      "Training Observation #2880: Loss = 0.009146312\n",
      "Training Observation #2890: Loss = 0.10284145\n",
      "Training Observation #2900: Loss = 1.3247284\n",
      "Training Observation #2910: Loss = 0.0015093235\n",
      "Training Observation #2920: Loss = 5.81911e-05\n",
      "Training Observation #2930: Loss = 1.8670889\n",
      "Training Observation #2940: Loss = 0.00081957044\n",
      "Training Observation #2950: Loss = 0.0045195622\n",
      "Training Observation #2960: Loss = 3.132951e-05\n",
      "Training Observation #2970: Loss = 1.3325042e-05\n",
      "Training Observation #2980: Loss = 2.5076642e-05\n",
      "Training Observation #2990: Loss = 0.00019156103\n",
      "Training Observation #3000: Loss = 0.0001102097\n",
      "Training Observation #3010: Loss = 0.72097224\n",
      "Training Observation #3020: Loss = 0.009169567\n",
      "Training Observation #3030: Loss = 3.48997e-06\n",
      "Training Observation #3040: Loss = 0.0036936428\n",
      "Training Observation #3050: Loss = 0.00068558165\n",
      "Training Observation #3060: Loss = 0.00015154331\n",
      "Training Observation #3070: Loss = 0.0021653408\n",
      "Training Observation #3080: Loss = 0.0077321595\n",
      "Training Observation #3090: Loss = 8.454388e-05\n",
      "Training Observation #3100: Loss = 0.004250719\n",
      "Training Observation #3110: Loss = 0.45100603\n",
      "Training Observation #3120: Loss = 2.1307453e-05\n",
      "Training Observation #3130: Loss = 1.898355e-05\n",
      "Training Observation #3140: Loss = 0.00016031101\n",
      "Training Observation #3150: Loss = 1.7806538e-05\n",
      "Training Observation #3160: Loss = 0.0057614627\n",
      "Training Observation #3170: Loss = 0.1866746\n",
      "Training Observation #3180: Loss = 0.46302804\n",
      "Training Observation #3190: Loss = 0.00085020316\n",
      "Training Observation #3200: Loss = 0.00059367495\n",
      "Training Observation #3210: Loss = 0.811056\n",
      "Training Observation #3220: Loss = 2.8197259e-05\n",
      "Training Observation #3230: Loss = 0.010037177\n",
      "Training Observation #3240: Loss = 1.0459105\n",
      "Training Observation #3250: Loss = 2.9853249\n",
      "Training Observation #3260: Loss = 0.04909006\n",
      "Training Observation #3270: Loss = 0.9762873\n",
      "Training Observation #3280: Loss = 0.52285415\n",
      "Training Observation #3290: Loss = 0.0024228762\n",
      "Training Observation #3300: Loss = 11.618245\n",
      "Training Observation #3310: Loss = 1.1890346\n",
      "Training Observation #3320: Loss = 0.89084256\n",
      "Training Observation #3330: Loss = 0.05927567\n",
      "Training Observation #3340: Loss = 0.0004535128\n",
      "Training Observation #3350: Loss = 5.5284276\n",
      "Training Observation #3360: Loss = 0.029420348\n",
      "Training Observation #3370: Loss = 0.058763526\n",
      "Training Observation #3380: Loss = 0.0006767258\n",
      "Training Observation #3390: Loss = 0.29863378\n",
      "Training Observation #3400: Loss = 0.00039937487\n",
      "Training Observation #3410: Loss = 0.0003801142\n",
      "Training Observation #3420: Loss = 2.9415634e-05\n",
      "Training Observation #3430: Loss = 0.0005761649\n",
      "Training Observation #3440: Loss = 0.00033026235\n",
      "Training Observation #3450: Loss = 3.8003454e-05\n",
      "Training Observation #3460: Loss = 0.31418616\n",
      "Training Observation #3470: Loss = 0.011778449\n",
      "Training Observation #3480: Loss = 0.2672993\n",
      "Training Observation #3490: Loss = 0.00029186494\n",
      "Training Observation #3500: Loss = 0.14049298\n",
      "Training Observation #3510: Loss = 0.00020647449\n",
      "Training Observation #3520: Loss = 8.0737904e-05\n",
      "Training Observation #3530: Loss = 0.8384458\n",
      "Training Observation #3540: Loss = 10.787103\n",
      "Training Observation #3550: Loss = 0.5082514\n",
      "Training Observation #3560: Loss = 11.762588\n",
      "Training Observation #3570: Loss = 5.031268e-05\n",
      "Training Observation #3580: Loss = 0.00799377\n",
      "Training Observation #3590: Loss = 5.4175363\n",
      "Training Observation #3600: Loss = 0.00402529\n",
      "Training Observation #3610: Loss = 0.5680573\n",
      "Training Observation #3620: Loss = 0.0013751618\n",
      "Training Observation #3630: Loss = 0.04156687\n",
      "Training Observation #3640: Loss = 3.928721e-07\n",
      "Training Observation #3650: Loss = 0.0043677962\n",
      "Training Observation #3660: Loss = 0.00919105\n",
      "Training Observation #3670: Loss = 0.0005663503\n",
      "Training Observation #3680: Loss = 0.00016169141\n",
      "Training Observation #3690: Loss = 1.1951356\n",
      "Training Observation #3700: Loss = 2.7851896\n",
      "Training Observation #3710: Loss = 0.6895742\n",
      "Training Observation #3720: Loss = 4.358884e-05\n",
      "Training Observation #3730: Loss = 0.00031641786\n",
      "Training Observation #3740: Loss = 0.0009245742\n",
      "Training Observation #3750: Loss = 1.244107e-05\n",
      "Training Observation #3760: Loss = 0.00015429757\n",
      "Training Observation #3770: Loss = 0.5676766\n",
      "Training Observation #3780: Loss = 2.7714033\n",
      "Training Observation #3790: Loss = 0.08706737\n",
      "Training Observation #3800: Loss = 0.3400215\n",
      "Training Observation #3810: Loss = 0.06658185\n",
      "Training Observation #3820: Loss = 0.017798971\n",
      "Training Observation #3830: Loss = 3.0540097\n",
      "Training Observation #3840: Loss = 0.81487006\n",
      "Training Observation #3850: Loss = 3.3661436e-05\n",
      "Training Observation #3860: Loss = 0.0011732938\n",
      "Training Observation #3870: Loss = 2.4075403e-05\n",
      "Training Observation #3880: Loss = 3.8881368e-05\n",
      "Training Observation #3890: Loss = 6.0363183\n",
      "Training Observation #3900: Loss = 0.00021160168\n",
      "Training Observation #3910: Loss = 0.0020163297\n",
      "Training Observation #3920: Loss = 1.9143956e-05\n",
      "Training Observation #3930: Loss = 0.00011795258\n",
      "Training Observation #3940: Loss = 3.5202145e-05\n",
      "Training Observation #3950: Loss = 0.6251665\n",
      "Training Observation #3960: Loss = 4.0402856\n",
      "Training Observation #3970: Loss = 14.049511\n",
      "Training Observation #3980: Loss = 1.6329805e-05\n",
      "Training Observation #3990: Loss = 0.012754189\n",
      "Training Observation #4000: Loss = 0.0012742277\n",
      "Training Observation #4010: Loss = 0.0011919804\n",
      "Training Observation #4020: Loss = 3.8401067e-05\n",
      "Training Observation #4030: Loss = 0.11978948\n",
      "Training Observation #4040: Loss = 0.0032779402\n",
      "Training Observation #4050: Loss = 0.22904639\n",
      "Training Observation #4060: Loss = 0.00095989846\n",
      "Training Observation #4070: Loss = 1.1250236\n",
      "Training Observation #4080: Loss = 0.00023302344\n",
      "Training Observation #4090: Loss = 0.0022100264\n",
      "Training Observation #4100: Loss = 0.0024042234\n",
      "Training Observation #4110: Loss = 0.0019310993\n",
      "Training Observation #4120: Loss = 0.03262241\n",
      "Training Observation #4130: Loss = 0.012214387\n",
      "Training Observation #4140: Loss = 0.0031520906\n",
      "Training Observation #4150: Loss = 1.4314463e-05\n",
      "Training Observation #4160: Loss = 2.9797823\n",
      "Training Observation #4170: Loss = 0.00012300414\n",
      "Training Observation #4180: Loss = 0.0032992815\n",
      "Training Observation #4190: Loss = 0.02025947\n",
      "Training Observation #4200: Loss = 7.4844146\n",
      "Training Observation #4210: Loss = 0.00023432696\n",
      "Training Observation #4220: Loss = 0.00013781135\n",
      "Training Observation #4230: Loss = 0.0032072542\n",
      "Training Observation #4240: Loss = 0.05218644\n",
      "Training Observation #4250: Loss = 4.535955e-06\n",
      "Training Observation #4260: Loss = 3.906445e-06\n",
      "Training Observation #4270: Loss = 0.002417233\n",
      "Training Observation #4280: Loss = 0.0007649462\n",
      "Training Observation #4290: Loss = 0.54510117\n",
      "Training Observation #4300: Loss = 0.00024155054\n",
      "Training Observation #4310: Loss = 2.7063757e-05\n",
      "Training Observation #4320: Loss = 8.454662e-05\n",
      "Training Observation #4330: Loss = 0.0024520748\n",
      "Training Observation #4340: Loss = 0.0018106796\n",
      "Training Observation #4350: Loss = 1.9022034\n",
      "Training Observation #4360: Loss = 0.0014355652\n",
      "Training Observation #4370: Loss = 3.2232914\n",
      "Training Observation #4380: Loss = 0.00011122286\n",
      "Training Observation #4390: Loss = 0.25806132\n",
      "Training Observation #4400: Loss = 3.3046534e-05\n",
      "Training Observation #4410: Loss = 0.03313842\n",
      "Training Observation #4420: Loss = 0.0002979747\n",
      "Training Observation #4430: Loss = 0.007320175\n",
      "Training Observation #4440: Loss = 0.0032860362\n",
      "Training Observation #4450: Loss = 0.00016983906\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "for ix,t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    sess.run(train_step,feed_dict={x_data:t,y_target:y_data})\n",
    "    temp_loss = sess.run(loss,feed_dict={x_data:t,y_target:y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    if (ix+1) % 10 == 0:\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + str(temp_loss))\n",
    "    [[temp_pred]] = sess.run(prediction,feed_dict={x_data:t,y_target:y_data})\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Test Set Accuracy\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy:0.8278026905829596\n"
     ]
    }
   ],
   "source": [
    "print('Getting Test Set Accuracy')\n",
    "test_acc_all = []\n",
    "for ix,t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    if (ix+1) % 50 == 0:\n",
    "        print('Test Observation #' + str(ix+1))\n",
    "    [[temp_pred]] = sess.run(prediction,feed_dict={x_data:t,y_target:y_data})\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "print('\\nOverall Test Accuracy:{}'.format(np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
